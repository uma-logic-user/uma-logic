# scripts/archive_manager.py
# UMA-Logic PRO - é‰„å£ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼ˆå®Œå…¨è‡ªå‹•åŒ–ç‰ˆï¼‰
# éšå±¤æ§‹é€ ä¿å­˜ + é«˜é€Ÿã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ©Ÿèƒ½

import json
import hashlib
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
import re

# --- å®šæ•° ---
DATA_DIR = Path("data")
ARCHIVE_DIR = DATA_DIR / "archive"
INDEX_FILE = ARCHIVE_DIR / "index.json"
CACHE_FILE = ARCHIVE_DIR / "cache.json"
RESULTS_PREFIX = "results_"


# --- ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ ---

class ArchiveIndex:
    """
    é«˜é€Ÿæ¤œç´¢ã®ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†
    å¹´ > æœˆ > æ—¥ > ç«¶é¦¬å ´ ã®éšå±¤æ§‹é€ ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç®¡ç†
    """
    
    def __init__(self):
        self.index: Dict = {
            "version": "2.0",
            "updated_at": "",
            "years": {},  # {year: {months: {month: {days: [...]}}}}
            "dates": {},  # {date_str: {path, race_count, venues, checksum, locked}}
            "venues": {},  # {venue: [date_str, ...]}
            "stats": {
                "total_dates": 0,
                "total_races": 0,
                "date_range": {"start": "", "end": ""}
            }
        }
        self.load()
    
    def load(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
        
        if INDEX_FILE.exists():
            try:
                with open(INDEX_FILE, 'r', encoding='utf-8') as f:
                    loaded = json.load(f)
                    if loaded.get("version") == "2.0":
                        self.index = loaded
                    else:
                        # æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                        self._migrate_from_v1(loaded)
            except Exception as e:
                print(f"[WARN] ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _migrate_from_v1(self, old_index: Dict):
        """v1ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        print("[INFO] ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’v2ã«ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸­...")
        
        for date_str, info in old_index.items():
            if isinstance(info, dict) and "locked" in info:
                self.index["dates"][date_str] = info
        
        self._rebuild_hierarchy()
        self.save()
    
    def _rebuild_hierarchy(self):
        """éšå±¤æ§‹é€ ã‚’å†æ§‹ç¯‰"""
        self.index["years"] = {}
        self.index["venues"] = {}
        
        for date_str in self.index["dates"].keys():
            self._add_to_hierarchy(date_str)
        
        self._update_stats()
    
    def _add_to_hierarchy(self, date_str: str):
        """æ—¥ä»˜ã‚’éšå±¤æ§‹é€ ã«è¿½åŠ """
        try:
            year = date_str[:4]
            month = date_str[4:6]
            day = date_str[6:8]
            
            # å¹´ > æœˆ > æ—¥ ã®éšå±¤
            if year not in self.index["years"]:
                self.index["years"][year] = {"months": {}}
            
            if month not in self.index["years"][year]["months"]:
                self.index["years"][year]["months"][month] = {"days": []}
            
            if day not in self.index["years"][year]["months"][month]["days"]:
                self.index["years"][year]["months"][month]["days"].append(day)
                self.index["years"][year]["months"][month]["days"].sort()
            
            # ç«¶é¦¬å ´ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            date_info = self.index["dates"].get(date_str, {})
            venues = date_info.get("venues", [])
            for venue in venues:
                if venue not in self.index["venues"]:
                    self.index["venues"][venue] = []
                if date_str not in self.index["venues"][venue]:
                    self.index["venues"][venue].append(date_str)
        except Exception:
            pass
    
    def _update_stats(self):
        """çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°"""
        dates = list(self.index["dates"].keys())
        
        self.index["stats"]["total_dates"] = len(dates)
        self.index["stats"]["total_races"] = sum(
            info.get("race_count", 0) for info in self.index["dates"].values()
        )
        
        if dates:
            dates.sort()
            self.index["stats"]["date_range"]["start"] = dates[0]
            self.index["stats"]["date_range"]["end"] = dates[-1]
    
    def save(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜"""
        self.index["updated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        with open(INDEX_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.index, f, ensure_ascii=False, indent=2)
    
    def is_archived(self, date_str: str) -> bool:
        """æŒ‡å®šæ—¥ä»˜ãŒã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ã‹ç¢ºèª"""
        return date_str in self.index["dates"] and self.index["dates"][date_str].get("locked", False)
    
    def add_entry(self, date_str: str, path: str, race_count: int, venues: List[str], checksum: str):
        """ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ """
        self.index["dates"][date_str] = {
            "path": str(path),
            "race_count": race_count,
            "venues": venues,
            "checksum": checksum,
            "locked": True,
            "archived_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        self._add_to_hierarchy(date_str)
        self._update_stats()
        self.save()
    
    def get_years(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªå¹´ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return sorted(self.index["years"].keys(), reverse=True)
    
    def get_months(self, year: str) -> List[str]:
        """æŒ‡å®šå¹´ã®æœˆãƒªã‚¹ãƒˆã‚’å–å¾—"""
        year_data = self.index["years"].get(year, {})
        return sorted(year_data.get("months", {}).keys())
    
    def get_days(self, year: str, month: str) -> List[str]:
        """æŒ‡å®šå¹´æœˆã®æ—¥ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        year_data = self.index["years"].get(year, {})
        month_data = year_data.get("months", {}).get(month, {})
        return sorted(month_data.get("days", []))
    
    def get_venues_for_date(self, date_str: str) -> List[str]:
        """æŒ‡å®šæ—¥ã®ç«¶é¦¬å ´ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self.index["dates"].get(date_str, {}).get("venues", [])
    
    def get_dates_for_venue(self, venue: str) -> List[str]:
        """æŒ‡å®šç«¶é¦¬å ´ã®é–‹å‚¬æ—¥ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return sorted(self.index["venues"].get(venue, []), reverse=True)
    
    def get_path(self, date_str: str) -> Optional[str]:
        """æŒ‡å®šæ—¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.index["dates"].get(date_str, {}).get("path")


# --- ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ ---

class ArchiveStorage:
    """
    éšå±¤å‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
    data/archive/YYYY/MM/DD/ å½¢å¼ã§ä¿å­˜
    """
    
    def __init__(self):
        self.index = ArchiveIndex()
        ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    
    def get_archive_path(self, date_str: str) -> Path:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‘ã‚¹ã‚’å–å¾—"""
        year = date_str[:4]
        month = date_str[4:6]
        day = date_str[6:8]
        return ARCHIVE_DIR / year / month / day
    
    def archive_results(self, date_str: str, data: Dict) -> Tuple[Path, str]:
        """
        çµæœãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ä¿å­˜
        Returns: (ä¿å­˜ãƒ‘ã‚¹, ãƒã‚§ãƒƒã‚¯ã‚µãƒ )
        """
        # æ—¢ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if self.index.is_archived(date_str):
            print(f"[SKIP] {date_str} ã¯æ—¢ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ã§ã™")
            existing_path = self.index.get_path(date_str)
            return Path(existing_path) if existing_path else None, ""
        
        archive_path = self.get_archive_path(date_str)
        archive_path.mkdir(parents=True, exist_ok=True)
        
        filepath = archive_path / f"results_{date_str}.json"
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
        data_for_checksum = {k: v for k, v in data.items() if k != "_meta"}
        data_str = json.dumps(data_for_checksum, ensure_ascii=False, sort_keys=True)
        checksum = hashlib.md5(data_str.encode()).hexdigest()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        races = data.get("races", [])
        venues = list(set(r.get("venue", "ä¸æ˜") for r in races))
        
        data["_meta"] = {
            "archived_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "checksum": checksum,
            "race_count": len(races),
            "venues": venues,
            "immutable": True
        }
        
        # ä¿å­˜
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # èª­ã¿å–ã‚Šå°‚ç”¨ã«è¨­å®š
        try:
            filepath.chmod(0o444)
        except:
            pass
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
        self.index.add_entry(date_str, str(filepath), len(races), venues, checksum)
        
        print(f"[ARCHIVED] {date_str} â†’ {filepath} ({len(races)}ãƒ¬ãƒ¼ã‚¹)")
        
        return filepath, checksum
    
    def load_from_archive(self, date_str: str) -> Optional[Dict]:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒ‘ã‚¹ã‚’å–å¾—
        path_str = self.index.get_path(date_str)
        
        if path_str:
            filepath = Path(path_str)
            if filepath.exists():
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        return json.load(f)
                except Exception as e:
                    print(f"[ERROR] èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({date_str}): {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥ãƒ‘ã‚¹ã‚’æ¢ç´¢
        archive_path = self.get_archive_path(date_str)
        filepath = archive_path / f"results_{date_str}.json"
        
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
        
        return None
    
    def sync_to_data_dir(self, date_str: str):
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰data/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼"""
        archive_data = self.load_from_archive(date_str)
        if not archive_data:
            return
        
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        target_path = DATA_DIR / f"results_{date_str}.json"
        
        if target_path.exists():
            return
        
        with open(target_path, 'w', encoding='utf-8') as f:
            json.dump(archive_data, f, ensure_ascii=False, indent=2)
    
    def verify_integrity(self, date_str: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’æ¤œè¨¼"""
        data = self.load_from_archive(date_str)
        if not data:
            return False
        
        meta = data.get("_meta", {})
        stored_checksum = meta.get("checksum", "")
        
        data_for_checksum = {k: v for k, v in data.items() if k != "_meta"}
        data_str = json.dumps(data_for_checksum, ensure_ascii=False, sort_keys=True)
        current_checksum = hashlib.md5(data_str.encode()).hexdigest()
        
        return stored_checksum == current_checksum


# --- è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ©Ÿèƒ½ ---

class AutoArchiver:
    """
    update_results.py ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ©Ÿèƒ½
    """
    
    def __init__(self):
        self.storage = ArchiveStorage()
    
    def archive_today_results(self):
        """æœ¬æ—¥ã®çµæœã‚’è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        today = datetime.now().strftime("%Y%m%d")
        return self.archive_date_results(today)
    
    def archive_date_results(self, date_str: str) -> bool:
        """æŒ‡å®šæ—¥ã®çµæœã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        # data/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        source_file = DATA_DIR / f"results_{date_str}.json"
        
        if not source_file.exists():
            print(f"[WARN] çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {source_file}")
            return False
        
        try:
            with open(source_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ä¿å­˜
            filepath, checksum = self.storage.archive_results(date_str, data)
            
            return filepath is not None
        
        except Exception as e:
            print(f"[ERROR] ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ ({date_str}): {e}")
            return False
    
    def archive_all_existing(self):
        """data/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å…¨çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        print("=" * 60)
        print("ğŸ“¦ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–")
        print("=" * 60)
        
        archived = 0
        skipped = 0
        
        for filepath in DATA_DIR.glob(f"{RESULTS_PREFIX}*.json"):
            date_str = filepath.stem.replace(RESULTS_PREFIX, "")[:8]
            
            if self.storage.index.is_archived(date_str):
                skipped += 1
                continue
            
            if self.archive_date_results(date_str):
                archived += 1
        
        print(f"\nâœ… å®Œäº†: {archived}ä»¶ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–, {skipped}ä»¶ã‚¹ã‚­ãƒƒãƒ—")
        return archived
    
    def rebuild_index(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰"""
        print("=" * 60)
        print("ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰")
        print("=" * 60)
        
        # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚¯ãƒªã‚¢
        self.storage.index.index["dates"] = {}
        self.storage.index.index["years"] = {}
        self.storage.index.index["venues"] = {}
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»
        count = 0
        for json_file in ARCHIVE_DIR.glob("**/*.json"):
            if json_file.name in ["index.json", "cache.json"]:
                continue
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æ—¥ä»˜ã‚’æŠ½å‡º
                date_match = re.search(r'results_(\d{8})', json_file.name)
                if not date_match:
                    continue
                
                date_str = date_match.group(1)
                races = data.get("races", [])
                venues = list(set(r.get("venue", "ä¸æ˜") for r in races))
                
                # ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
                data_for_checksum = {k: v for k, v in data.items() if k != "_meta"}
                data_str = json.dumps(data_for_checksum, ensure_ascii=False, sort_keys=True)
                checksum = hashlib.md5(data_str.encode()).hexdigest()
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
                self.storage.index.add_entry(date_str, str(json_file), len(races), venues, checksum)
                count += 1
                
            except Exception as e:
                print(f"[WARN] {json_file}: {e}")
                continue
        
        print(f"\nâœ… {count}ä»¶ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å†æ§‹ç¯‰ã—ã¾ã—ãŸ")
        return count


# --- UIç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ ---

class ArchiveDataLoader:
    """
    app_commercial.py ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    é«˜é€Ÿãªéšå±¤æ¤œç´¢ã‚’æä¾›
    """
    
    def __init__(self):
        self.storage = ArchiveStorage()
        self._cache: Dict[str, Dict] = {}
    
    def get_available_years(self) -> List[int]:
        """åˆ©ç”¨å¯èƒ½ãªå¹´ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        years = self.storage.index.get_years()
        return [int(y) for y in years]
    
    def get_available_months(self, year: int) -> List[int]:
        """æŒ‡å®šå¹´ã®æœˆãƒªã‚¹ãƒˆã‚’å–å¾—"""
        months = self.storage.index.get_months(str(year))
        return [int(m) for m in months]
    
    def get_available_days(self, year: int, month: int) -> List[int]:
        """æŒ‡å®šå¹´æœˆã®æ—¥ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        days = self.storage.index.get_days(str(year), f"{month:02d}")
        return [int(d) for d in days]
    
    def get_venues_for_date(self, year: int, month: int, day: int) -> List[str]:
        """æŒ‡å®šæ—¥ã®ç«¶é¦¬å ´ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        date_str = f"{year}{month:02d}{day:02d}"
        return self.storage.index.get_venues_for_date(date_str)
    
    def load_races_for_date(self, year: int, month: int, day: int) -> List[Dict]:
        """æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        date_str = f"{year}{month:02d}{day:02d}"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if date_str in self._cache:
            return self._cache[date_str].get("races", [])
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰èª­ã¿è¾¼ã¿
        data = self.storage.load_from_archive(date_str)
        
        if not data:
            # data/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            filepath = DATA_DIR / f"results_{date_str}.json"
            if filepath.exists():
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                except:
                    return []
        
        if data:
            self._cache[date_str] = data
            return data.get("races", [])
        
        return []
    
    def load_races_for_venue(self, year: int, month: int, day: int, venue: str) -> List[Dict]:
        """æŒ‡å®šæ—¥ãƒ»ç«¶é¦¬å ´ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        all_races = self.load_races_for_date(year, month, day)
        return [r for r in all_races if r.get("venue") == venue]
    
    def get_stats(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return self.storage.index.index.get("stats", {})
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self._cache = {}


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---

def main():
    import sys
    
    print("=" * 60)
    print("ğŸ“¦ UMA-Logic PRO - ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼")
    print("=" * 60)
    
    archiver = AutoArchiver()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "--archive-all":
            archiver.archive_all_existing()
        
        elif command == "--rebuild-index":
            archiver.rebuild_index()
        
        elif command == "--archive-date" and len(sys.argv) > 2:
            date_str = sys.argv[2]
            archiver.archive_date_results(date_str)
        
        elif command == "--stats":
            loader = ArchiveDataLoader()
            stats = loader.get_stats()
            print(f"\nğŸ“Š ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–çµ±è¨ˆ:")
            print(f"  ç·æ—¥æ•°: {stats.get('total_dates', 0)}")
            print(f"  ç·ãƒ¬ãƒ¼ã‚¹æ•°: {stats.get('total_races', 0)}")
            date_range = stats.get('date_range', {})
            print(f"  æœŸé–“: {date_range.get('start', '-')} ã€œ {date_range.get('end', '-')}")
        
        else:
            print("ä½¿ç”¨æ–¹æ³•:")
            print("  --archive-all      : å…¨æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–")
            print("  --rebuild-index    : ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰")
            print("  --archive-date DATE: æŒ‡å®šæ—¥ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–")
            print("  --stats            : çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º")
    
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ¬æ—¥ã®çµæœã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        archiver.archive_today_results()
    
    print("\nâœ… å‡¦ç†å®Œäº†")


if __name__ == "__main__":
    main()
