# scripts/fetch_historical_data.py
# UMA-Logic Pro - éå»ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
# netkeibaã®ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰é–‹å‚¬æ—¥ã‚’å–å¾—ã—ã€ãƒ¬ãƒ¼ã‚¹çµæœã‚’åé›†

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set
import sys

# --- å®šæ•° ---
DATA_DIR = Path("data")
ARCHIVE_DIR = DATA_DIR / "archive"
RESULTS_PREFIX = "results_"

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 3
REQUEST_TIMEOUT = 30
REQUEST_INTERVAL = 1.5

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
}


def fetch_with_retry(url: str, encoding: str = None) -> Optional[str]:
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            
            # æ–‡å­—ã‚³ãƒ¼ãƒ‰å‡¦ç†
            if encoding:
                return response.content.decode(encoding, errors='replace')
            
            # è‡ªå‹•æ¤œå‡º
            content = response.content[:2000].lower()
            if b'euc-jp' in content:
                return response.content.decode('euc-jp', errors='replace')
            elif b'shift_jis' in content:
                return response.content.decode('shift_jis', errors='replace')
            
            return response.content.decode('utf-8', errors='replace')
            
        except requests.RequestException as e:
            print(f"    [WARN] ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— ({attempt + 1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
    return None


def parse_number(text: str) -> int:
    """æ•°å€¤æŠ½å‡º"""
    if not text:
        return 0
    text = text.replace(',', '').replace('å††', '').replace('Â¥', '')
    nums = re.findall(r'\d+', text)
    return int(nums[0]) if nums else 0


def parse_float(text: str) -> float:
    """å°æ•°æŠ½å‡º"""
    if not text:
        return 0.0
    nums = re.findall(r'[\d.]+', text)
    return float(nums[0]) if nums else 0.0


def get_race_dates_from_calendar(year: int, month: int) -> List[str]:
    """
    netkeibaã®ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰é–‹å‚¬æ—¥ã‚’å–å¾—
    """
    url = f"https://race.netkeiba.com/top/calendar.html?year={year}&month={month}"
    
    html = fetch_with_retry(url)
    if not html:
        return []
    
    soup = BeautifulSoup(html, 'lxml')
    dates = []
    
    # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã®ãƒªãƒ³ã‚¯ã‹ã‚‰é–‹å‚¬æ—¥ã‚’æŠ½å‡º
    for link in soup.find_all('a', href=True):
        href = link['href']
        match = re.search(r'kaisai_date=(\d{8})', href)
        if match:
            date_str = match.group(1)
            if date_str not in dates:
                dates.append(date_str)
    
    return sorted(dates)


def get_race_ids_for_date(date_str: str) -> List[str]:
    """
    æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’å–å¾—
    """
    url = f"https://race.netkeiba.com/top/race_list.html?kaisai_date={date_str}"
    
    html = fetch_with_retry(url)
    if not html:
        return []
    
    soup = BeautifulSoup(html, 'lxml')
    race_ids = []
    
    # ãƒ¬ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‹ã‚‰IDã‚’æŠ½å‡º
    for link in soup.find_all('a', href=True):
        href = link['href']
        match = re.search(r'race_id=(\d+)', href)
        if match:
            race_id = match.group(1)
            if race_id not in race_ids and len(race_id) >= 12:
                race_ids.append(race_id)
    
    return race_ids


def fetch_race_result(race_id: str) -> Optional[Dict]:
    """
    ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—ï¼ˆrace.netkeiba.com/race/result.htmlï¼‰
    """
    url = f"https://race.netkeiba.com/race/result.html?race_id={race_id}"
    
    html = fetch_with_retry(url)
    if not html:
        return None
    
    soup = BeautifulSoup(html, 'lxml')
    
    # ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if 'ãƒ¬ãƒ¼ã‚¹çµæœ' not in html and 'ç€é †' not in html:
        return None
    
    race_data = {
        "race_id": race_id,
        "race_num": 0,
        "race_name": "",
        "venue": "",
        "top3": [],
        "all_results": [],
        "payouts": {}
    }
    
    # ãƒ¬ãƒ¼ã‚¹ç•ªå·
    race_num_elem = soup.select_one('.RaceNum')
    if race_num_elem:
        race_data["race_num"] = parse_number(race_num_elem.get_text())
    
    # ãƒ¬ãƒ¼ã‚¹å
    race_name_elem = soup.select_one('.RaceName')
    if race_name_elem:
        race_data["race_name"] = race_name_elem.get_text(strip=True)
    
    # ç«¶é¦¬å ´
    venue_elem = soup.select_one('.RaceData02 span')
    if venue_elem:
        venue_text = venue_elem.get_text(strip=True)
        venue_match = re.search(r'[0-9]+å›(.+?)[0-9]+æ—¥', venue_text)
        if venue_match:
            race_data["venue"] = venue_match.group(1)
        else:
            # ç«¶é¦¬å ´åã‚’æŠ½å‡º
            for v in ["æ±äº¬", "ä¸­å±±", "é˜ªç¥", "äº¬éƒ½", "ä¸­äº¬", "å°å€‰", "æ–°æ½Ÿ", "ç¦å³¶", "æœ­å¹Œ", "å‡½é¤¨"]:
                if v in venue_text:
                    race_data["venue"] = v
                    break
    
    # ç€é †ãƒ†ãƒ¼ãƒ–ãƒ«
    result_table = soup.select_one('.ResultTableWrap table, table.RaceTable01')
    if result_table:
        rows = result_table.select('tr')
        
        for row in rows:
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if row.select('th'):
                continue
            
            cells = row.select('td')
            if len(cells) < 5:
                continue
            
            try:
                # ç€é †
                rank_elem = row.select_one('.Rank, td:first-child')
                rank = parse_number(rank_elem.get_text()) if rank_elem else 0
                if rank == 0:
                    continue
                
                # é¦¬ç•ª
                umaban_elem = row.select_one('.Umaban, .Waku span')
                umaban = 0
                if umaban_elem:
                    umaban = parse_number(umaban_elem.get_text())
                else:
                    # 2ç•ªç›®ã‹3ç•ªç›®ã®ã‚»ãƒ«ã‹ã‚‰å–å¾—
                    for i in [1, 2]:
                        if i < len(cells):
                            umaban = parse_number(cells[i].get_text())
                            if 1 <= umaban <= 18:
                                break
                
                # é¦¬å
                horse_name_elem = row.select_one('.Horse_Name a, .HorseName a')
                horse_name = horse_name_elem.get_text(strip=True) if horse_name_elem else ""
                
                # é¨æ‰‹
                jockey_elem = row.select_one('.Jockey a')
                jockey = jockey_elem.get_text(strip=True) if jockey_elem else ""
                
                # ã‚¿ã‚¤ãƒ 
                time_elem = row.select_one('.Time .RaceTime, .Time')
                race_time = ""
                if time_elem:
                    time_text = time_elem.get_text(strip=True)
                    time_match = re.search(r'[\d:\.]+', time_text)
                    if time_match:
                        race_time = time_match.group()
                
                # ä¸ŠãŒã‚Š3F
                last3f_elem = row.select_one('.Time .RapTime')
                last3f = last3f_elem.get_text(strip=True) if last3f_elem else ""
                
                # ã‚ªãƒƒã‚º
                odds_elem = row.select_one('.Odds span, .Odds')
                odds = parse_float(odds_elem.get_text()) if odds_elem else 0.0
                
                if not horse_name:
                    continue
                
                horse_result = {
                    "ç€é †": rank,
                    "é¦¬ç•ª": umaban,
                    "é¦¬å": horse_name,
                    "é¨æ‰‹": jockey,
                    "ã‚¿ã‚¤ãƒ ": race_time,
                    "ä¸ŠãŒã‚Š3F": last3f,
                    "ã‚ªãƒƒã‚º": odds
                }
                
                race_data["all_results"].append(horse_result)
                
                if rank <= 3:
                    race_data["top3"].append(horse_result)
                    
            except Exception as e:
                continue
    
    # ãƒ‡ãƒ¼ã‚¿ãŒãªã‘ã‚Œã°None
    if not race_data["all_results"]:
        return None
    
    race_data["top3"] = sorted(race_data["top3"], key=lambda x: x.get("ç€é †", 99))[:3]
    race_data["all_results"] = sorted(race_data["all_results"], key=lambda x: x.get("ç€é †", 99))
    
    # æ‰•æˆ»é‡‘ãƒ†ãƒ¼ãƒ–ãƒ«
    payout_section = soup.select_one('.FullWrap, .PaybackWrap, #All_Result_PayBack')
    if payout_section:
        # å˜å‹
        tansho = payout_section.select_one('.Tansho .Value, [class*="Tansho"] .Payout')
        if tansho:
            race_data["payouts"]["å˜å‹"] = parse_number(tansho.get_text())
        
        # é¦¬é€£
        umaren = payout_section.select_one('.Umaren .Value, [class*="Umaren"] .Payout')
        if umaren:
            race_data["payouts"]["é¦¬é€£"] = parse_number(umaren.get_text())
        
        # é¦¬å˜
        umatan = payout_section.select_one('.Umatan .Value, [class*="Umatan"] .Payout')
        if umatan:
            race_data["payouts"]["é¦¬å˜"] = parse_number(umatan.get_text())
        
        # ä¸‰é€£è¤‡
        sanrenpuku = payout_section.select_one('.Fuku3 .Value, [class*="Sanrenpuku"] .Payout')
        if sanrenpuku:
            race_data["payouts"]["ä¸‰é€£è¤‡"] = parse_number(sanrenpuku.get_text())
        
        # ä¸‰é€£å˜
        sanrentan = payout_section.select_one('.Tan3 .Value, [class*="Sanrentan"] .Payout')
        if sanrentan:
            race_data["payouts"]["ä¸‰é€£å˜"] = parse_number(sanrentan.get_text())
    
    # åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ‰•æˆ»é‡‘å–å¾—
    if not race_data["payouts"]:
        payout_rows = soup.select('.Payout tr, .PaybackTable tr')
        for row in payout_rows:
            try:
                th = row.select_one('th')
                td = row.select_one('td')
                if th and td:
                    bet_type = th.get_text(strip=True)
                    payout = parse_number(td.get_text())
                    
                    if "å˜å‹" in bet_type:
                        race_data["payouts"]["å˜å‹"] = payout
                    elif "é¦¬é€£" in bet_type:
                        race_data["payouts"]["é¦¬é€£"] = payout
                    elif "é¦¬å˜" in bet_type:
                        race_data["payouts"]["é¦¬å˜"] = payout
                    elif "ä¸‰é€£è¤‡" in bet_type or "3é€£è¤‡" in bet_type:
                        race_data["payouts"]["ä¸‰é€£è¤‡"] = payout
                    elif "ä¸‰é€£å˜" in bet_type or "3é€£å˜" in bet_type:
                        race_data["payouts"]["ä¸‰é€£å˜"] = payout
            except:
                continue
    
    return race_data


def file_exists_and_valid(filepath: Path) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ã‹ç¢ºèª"""
    if not filepath.exists():
        return False
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            races = data.get("races", [])
            if races and len(races) >= 6:  # æœ€ä½6ãƒ¬ãƒ¼ã‚¹ä»¥ä¸Š
                if races[0].get("top3") or races[0].get("all_results"):
                    return True
    except:
        pass
    
    return False


def save_results(results: List[Dict], target_date: datetime):
    """çµæœãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    
    date_str = target_date.strftime("%Y%m%d")
    filepath = DATA_DIR / f"{RESULTS_PREFIX}{date_str}.json"
    archive_path = ARCHIVE_DIR / f"{RESULTS_PREFIX}{date_str}.json"
    
    output_data = {
        "date": target_date.strftime("%Y-%m-%d"),
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "races": results
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"    [SAVED] {len(results)}ãƒ¬ãƒ¼ã‚¹")
    return True


def main():
    print("=" * 60)
    print("ğŸ‡ UMA-Logic Pro - éå»ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰")
    print("=" * 60)
    
    # å¼•æ•°ã§å¹´ã‚’æŒ‡å®šå¯èƒ½
    if len(sys.argv) > 1:
        try:
            years = [int(y) for y in sys.argv[1:]]
        except ValueError:
            print("[ERROR] å¹´ã¯æ•°å­—ã§æŒ‡å®šã—ã¦ãã ã•ã„")
            sys.exit(1)
    else:
        current_year = datetime.now().year
        years = [current_year - 1, current_year]
    
    print(f"[INFO] å¯¾è±¡å¹´: {years}")
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    
    total_saved = 0
    total_skipped = 0
    total_failed = 0
    
    for year in years:
        print(f"\n{'='*50}")
        print(f"ğŸ“… {year}å¹´ã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
        print(f"{'='*50}")
        
        for month in range(1, 13):
            print(f"\n[INFO] {year}å¹´{month}æœˆã®é–‹å‚¬æ—¥ã‚’å–å¾—ä¸­...")
            
            # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰é–‹å‚¬æ—¥ã‚’å–å¾—
            dates = get_race_dates_from_calendar(year, month)
            
            if not dates:
                print(f"  é–‹å‚¬æ—¥ãªã—")
                continue
            
            print(f"  {len(dates)}æ—¥ã®é–‹å‚¬æ—¥ã‚’ç™ºè¦‹")
            
            for date_str in dates:
                target_date = datetime.strptime(date_str, "%Y%m%d")
                filepath = DATA_DIR / f"{RESULTS_PREFIX}{date_str}.json"
                
                # æœªæ¥ã®æ—¥ä»˜ã¯ã‚¹ã‚­ãƒƒãƒ—
                if target_date > datetime.now():
                    continue
                
                weekday_jp = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
                print(f"\n  [{date_str}] {target_date.month}/{target_date.day}({weekday_jp[target_date.weekday()]})")
                
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
                if file_exists_and_valid(filepath):
                    print(f"    [SKIP] æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š")
                    total_skipped += 1
                    continue
                
                # ãƒ¬ãƒ¼ã‚¹IDå–å¾—
                race_ids = get_race_ids_for_date(date_str)
                
                if not race_ids:
                    print(f"    [WARN] ãƒ¬ãƒ¼ã‚¹IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    total_failed += 1
                    time.sleep(1)
                    continue
                
                print(f"    {len(race_ids)}ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—ä¸­...")
                
                # å„ãƒ¬ãƒ¼ã‚¹ã®çµæœã‚’å–å¾—
                results = []
                for race_id in race_ids:
                    result = fetch_race_result(race_id)
                    if result and result.get("all_results"):
                        results.append(result)
                    time.sleep(REQUEST_INTERVAL)
                
                # ä¿å­˜
                if results:
                    save_results(results, target_date)
                    total_saved += 1
                else:
                    print(f"    [WARN] æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãªã—")
                    total_failed += 1
                
                # æ—¥ä»˜é–“ã®å¾…æ©Ÿ
                time.sleep(1)
    
    print("\n" + "=" * 60)
    print(f"âœ… å‡¦ç†å®Œäº†")
    print(f"   æ–°è¦ä¿å­˜: {total_saved}æ—¥åˆ†")
    print(f"   ã‚¹ã‚­ãƒƒãƒ—: {total_skipped}æ—¥åˆ†ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Šï¼‰")
    print(f"   å¤±æ•—: {total_failed}æ—¥åˆ†")
    print("=" * 60)


if __name__ == "__main__":
    main()
