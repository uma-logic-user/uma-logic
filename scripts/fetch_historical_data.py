# scripts/fetch_historical_data.py
# UMA-Logic Pro - éå»ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆdb.netkeiba.comç‰ˆï¼‰
# å‹•ä½œç¢ºèªæ¸ˆã¿

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import sys

# --- å®šæ•° ---
DATA_DIR = Path("data")
ARCHIVE_DIR = DATA_DIR / "archive"
RESULTS_PREFIX = "results_"

MAX_RETRIES = 3
RETRY_DELAY = 3
REQUEST_TIMEOUT = 30
REQUEST_INTERVAL = 1.5

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
}


def fetch_with_retry(url: str, encoding: str = 'euc-jp') -> Optional[str]:
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            return response.content.decode(encoding, errors='replace')
        except requests.RequestException as e:
            print(f"    [WARN] ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— ({attempt + 1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
    return None


def parse_number(text: str) -> int:
    if not text:
        return 0
    text = text.replace(',', '').replace('å††', '')
    nums = re.findall(r'\d+', text)
    return int(nums[0]) if nums else 0


def parse_float(text: str) -> float:
    if not text:
        return 0.0
    nums = re.findall(r'[\d.]+', text)
    return float(nums[0]) if nums else 0.0


def get_race_dates_from_calendar(year: int, month: int) -> List[str]:
    """race.netkeiba.comã®ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰é–‹å‚¬æ—¥ã‚’å–å¾—"""
    url = f"https://race.netkeiba.com/top/calendar.html?year={year}&month={month}"
    html = fetch_with_retry(url, 'utf-8')
    if not html:
        return []
    
    soup = BeautifulSoup(html, 'lxml')
    dates = []
    for link in soup.find_all('a', href=True):
        match = re.search(r'kaisai_date=(\d{8})', link['href'])
        if match:
            date_str = match.group(1)
            if date_str not in dates and date_str.startswith(str(year)):
                dates.append(date_str)
    return sorted(dates)


def get_race_ids_for_date(date_str: str) -> List[str]:
    """db.netkeiba.comã‹ã‚‰æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’å–å¾—"""
    url = f"https://db.netkeiba.com/race/list/{date_str}/"
    html = fetch_with_retry(url)
    if not html:
        return []
    
    soup = BeautifulSoup(html, 'lxml')
    race_ids = []
    for link in soup.select('a[href*="/race/"]'):
        match = re.search(r'/race/(\d{12})/', link.get('href', ''))
        if match and match.group(1) not in race_ids:
            race_ids.append(match.group(1))
    return sorted(race_ids)


def fetch_race_result(race_id: str) -> Optional[Dict]:
    """db.netkeiba.comã‹ã‚‰ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—"""
    url = f"https://db.netkeiba.com/race/{race_id}/"
    html = fetch_with_retry(url)
    if not html:
        return None
    
    soup = BeautifulSoup(html, 'lxml')
    
    race_data = {
        "race_id": race_id,
        "race_num": int(race_id[-2:]),
        "race_name": "",
        "venue": "",
        "top3": [],
        "all_results": [],
        "payouts": {}
    }
    
    # ãƒ¬ãƒ¼ã‚¹å
    race_name_elem = soup.select_one('.racedata h1, .data_intro h1')
    if race_name_elem:
        race_data["race_name"] = race_name_elem.get_text(strip=True)
    
    # ç«¶é¦¬å ´
    race_info = soup.select_one('.racedata, .data_intro')
    if race_info:
        info_text = race_info.get_text()
        for v in ["æ±äº¬", "ä¸­å±±", "é˜ªç¥", "äº¬éƒ½", "ä¸­äº¬", "å°å€‰", "æ–°æ½Ÿ", "ç¦å³¶", "æœ­å¹Œ", "å‡½é¤¨"]:
            if v in info_text:
                race_data["venue"] = v
                break
    
    # ç€é †ãƒ†ãƒ¼ãƒ–ãƒ«
    result_table = soup.select_one('table.race_table_01')
    if result_table:
        rows = result_table.select('tr')
        for row in rows[1:]:
            try:
                cells = row.select('td')
                if len(cells) < 10:
                    continue
                
                rank = parse_number(cells[0].get_text(strip=True))
                if rank == 0:
                    continue
                
                horse_name_elem = cells[3].select_one('a')
                horse_name = horse_name_elem.get_text(strip=True) if horse_name_elem else cells[3].get_text(strip=True)
                
                jockey_elem = cells[6].select_one('a') if len(cells) > 6 else None
                jockey = jockey_elem.get_text(strip=True) if jockey_elem else ""
                
                horse_result = {
                    "ç€é †": rank,
                    "é¦¬ç•ª": parse_number(cells[2].get_text(strip=True)),
                    "é¦¬å": horse_name,
                    "é¨æ‰‹": jockey,
                    "ã‚¿ã‚¤ãƒ ": cells[7].get_text(strip=True) if len(cells) > 7 else "",
                    "ä¸ŠãŒã‚Š3F": cells[11].get_text(strip=True) if len(cells) > 11 else "",
                    "ã‚ªãƒƒã‚º": parse_float(cells[12].get_text(strip=True)) if len(cells) > 12 else 0.0
                }
                
                race_data["all_results"].append(horse_result)
                if rank <= 3:
                    race_data["top3"].append(horse_result)
            except:
                continue
    
    if not race_data["all_results"]:
        return None
    
    # æ‰•æˆ»é‡‘
    for table in soup.select('table.pay_table_01'):
        for row in table.select('tr'):
            try:
                th = row.select_one('th')
                tds = row.select('td')
                if th and len(tds) >= 2:
                    bet_type = th.get_text(strip=True)
                    payout = parse_number(tds[1].get_text(strip=True))
                    if "å˜å‹" in bet_type:
                        race_data["payouts"]["å˜å‹"] = payout
                    elif "è¤‡å‹" in bet_type:
                        race_data["payouts"]["è¤‡å‹"] = payout
                    elif "æ é€£" in bet_type:
                        race_data["payouts"]["æ é€£"] = payout
                    elif "é¦¬é€£" in bet_type:
                        race_data["payouts"]["é¦¬é€£"] = payout
                    elif "é¦¬å˜" in bet_type:
                        race_data["payouts"]["é¦¬å˜"] = payout
                    elif "ãƒ¯ã‚¤ãƒ‰" in bet_type:
                        race_data["payouts"]["ãƒ¯ã‚¤ãƒ‰"] = payout
                    elif "ä¸‰é€£è¤‡" in bet_type:
                        race_data["payouts"]["ä¸‰é€£è¤‡"] = payout
                    elif "ä¸‰é€£å˜" in bet_type:
                        race_data["payouts"]["ä¸‰é€£å˜"] = payout
            except:
                continue
    
    return race_data


def file_exists_and_valid(filepath: Path) -> bool:
    if not filepath.exists():
        return False
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            races = data.get("races", [])
            return len(races) >= 6 and races[0].get("all_results")
    except:
        return False


def save_results(results: List[Dict], target_date: datetime):
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    
    date_str = target_date.strftime("%Y%m%d")
    filepath = DATA_DIR / f"{RESULTS_PREFIX}{date_str}.json"
    archive_path = ARCHIVE_DIR / f"{RESULTS_PREFIX}{date_str}.json"
    
    output_data = {
        "date": target_date.strftime("%Y-%m-%d"),
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "races": results
    }
    
    for path in [filepath, archive_path]:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"    [SAVED] {len(results)}ãƒ¬ãƒ¼ã‚¹ â†’ {filepath.name}")


def main():
    print("=" * 60)
    print("ğŸ‡ UMA-Logic Pro - éå»ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—ï¼ˆdb.netkeiba.comç‰ˆï¼‰")
    print("=" * 60)
    
    if len(sys.argv) > 1:
        try:
            years = [int(y) for y in sys.argv[1:]]
        except ValueError:
            print("[ERROR] å¹´ã¯æ•°å­—ã§æŒ‡å®šã—ã¦ãã ã•ã„")
            sys.exit(1)
    else:
        current_year = datetime.now().year
        years = [current_year - 1, current_year]
    
    print(f"[INFO] å¯¾è±¡å¹´: {years}")
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    
    total_saved = 0
    total_skipped = 0
    total_failed = 0
    
    for year in years:
        print(f"\n{'='*50}")
        print(f"ğŸ“… {year}å¹´ã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
        print(f"{'='*50}")
        
        for month in range(1, 13):
            print(f"\n[INFO] {year}å¹´{month}æœˆ")
            
            dates = get_race_dates_from_calendar(year, month)
            if not dates:
                print(f"  é–‹å‚¬æ—¥ãªã—")
                continue
            
            print(f"  {len(dates)}æ—¥ã®é–‹å‚¬æ—¥")
            
            for date_str in dates:
                target_date = datetime.strptime(date_str, "%Y%m%d")
                filepath = DATA_DIR / f"{RESULTS_PREFIX}{date_str}.json"
                
                if target_date > datetime.now():
                    continue
                
                weekday_jp = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
                print(f"\n  [{date_str}] {target_date.month}/{target_date.day}({weekday_jp[target_date.weekday()]})")
                
                if file_exists_and_valid(filepath):
                    print(f"    [SKIP] æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š")
                    total_skipped += 1
                    continue
                
                race_ids = get_race_ids_for_date(date_str)
                if not race_ids:
                    print(f"    [WARN] ãƒ¬ãƒ¼ã‚¹IDãªã—")
                    total_failed += 1
                    time.sleep(1)
                    continue
                
                print(f"    {len(race_ids)}ãƒ¬ãƒ¼ã‚¹å–å¾—ä¸­...")
                
                results = []
                for race_id in race_ids:
                    result = fetch_race_result(race_id)
                    if result:
                        results.append(result)
                    time.sleep(REQUEST_INTERVAL)
                
                if results:
                    save_results(results, target_date)
                    total_saved += 1
                else:
                    print(f"    [WARN] æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãªã—")
                    total_failed += 1
                
                time.sleep(1)
    
    print("\n" + "=" * 60)
    print(f"âœ… å‡¦ç†å®Œäº†")
    print(f"   æ–°è¦ä¿å­˜: {total_saved}æ—¥åˆ†")
    print(f"   ã‚¹ã‚­ãƒƒãƒ—: {total_skipped}æ—¥åˆ†")
    print(f"   å¤±æ•—: {total_failed}æ—¥åˆ†")
    print("=" * 60)


if __name__ == "__main__":
    main()
