# scripts/fetch_historical_data.py
# UMA-Logic Pro - éå»ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# éå»2å¹´åˆ†ã®ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—ã—ã€ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¨ã—ã¦ä¿å­˜

import requests
from bs4 import BeautifulSoup
import json
import os
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import sys

# --- å®šæ•° ---
BASE_URL = "https://race.netkeiba.com"
RESULT_URL = "https://race.netkeiba.com/race/result.html"
CALENDAR_URL = "https://race.netkeiba.com/top/calendar.html"

DATA_DIR = Path("data" )
ARCHIVE_DIR = DATA_DIR / "archive"
RESULTS_PREFIX = "results_"

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 3
REQUEST_TIMEOUT = 20
REQUEST_INTERVAL = 2.0  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚é•·ã‚ã«

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
}


def fetch_with_retry(url: str, params: dict = None) -> Optional[requests.Response]:
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(url, params=params, headers=HEADERS, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            print(f"  [WARN] ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
    return None


def detect_encoding(response: requests.Response) -> str:
    """æ–‡å­—ã‚³ãƒ¼ãƒ‰æ¤œå‡º"""
    content = response.content[:1000].lower()
    if b'euc-jp' in content:
        return 'euc-jp'
    elif b'shift_jis' in content:
        return 'shift_jis'
    return 'utf-8'


def parse_number(text: str) -> int:
    """æ•°å€¤æŠ½å‡º"""
    if not text:
        return 0
    nums = re.findall(r'[\d,]+', text.replace(',', ''))
    return int(nums[0]) if nums else 0


def parse_float(text: str) -> float:
    """å°æ•°æŠ½å‡º"""
    if not text:
        return 0.0
    nums = re.findall(r'[\d.]+', text)
    return float(nums[0]) if nums else 0.0


def get_race_dates_for_year(year: int) -> List[datetime]:
    """æŒ‡å®šå¹´ã®é–‹å‚¬æ—¥ãƒªã‚¹ãƒˆã‚’å–å¾—"""
    print(f"[INFO] {year}å¹´ã®é–‹å‚¬æ—¥ã‚’å–å¾—ä¸­...")
    
    race_dates = []
    
    for month in range(1, 13):
        url = f"{CALENDAR_URL}?year={year}&month={month}"
        response = fetch_with_retry(url)
        
        if not response:
            continue
        
        encoding = detect_encoding(response)
        soup = BeautifulSoup(response.content.decode(encoding, errors='replace'), 'lxml')
        
        # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰é–‹å‚¬æ—¥ã‚’æŠ½å‡º
        for link in soup.find_all('a', href=True):
            href = link['href']
            if 'kaisai_date=' in href:
                match = re.search(r'kaisai_date=(\d{8})', href)
                if match:
                    try:
                        date = datetime.strptime(match.group(1), "%Y%m%d")
                        if date not in race_dates:
                            race_dates.append(date)
                    except ValueError:
                        continue
        
        time.sleep(0.5)
    
    race_dates.sort()
    print(f"[INFO] {year}å¹´: {len(race_dates)}æ—¥ã®é–‹å‚¬æ—¥ã‚’å–å¾—")
    return race_dates


def get_race_ids_for_date(target_date: datetime) -> List[str]:
    """æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’å–å¾—"""
    date_str = target_date.strftime("%Y%m%d")
    url = f"https://race.netkeiba.com/top/race_list.html?kaisai_date={date_str}"
    
    response = fetch_with_retry(url )
    if not response:
        return []
    
    encoding = detect_encoding(response)
    soup = BeautifulSoup(response.content.decode(encoding, errors='replace'), 'lxml')
    
    race_ids = []
    for link in soup.find_all('a', href=True):
        href = link['href']
        if 'race_id=' in href:
            match = re.search(r'race_id=(\d+)', href)
            if match:
                race_id = match.group(1)
                if race_id not in race_ids:
                    race_ids.append(race_id)
    
    return race_ids


def fetch_race_result(race_id: str) -> Optional[Dict]:
    """ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—"""
    url = f"{RESULT_URL}?race_id={race_id}"
    
    response = fetch_with_retry(url)
    if not response:
        return None
    
    encoding = detect_encoding(response)
    soup = BeautifulSoup(response.content.decode(encoding, errors='replace'), 'lxml')
    
    race_data = {
        "race_id": race_id,
        "race_num": 0,
        "race_name": "",
        "venue": "",
        "top3": [],
        "all_results": [],
        "payouts": {}
    }
    
    # ãƒ¬ãƒ¼ã‚¹ç•ªå·
    race_num_elem = soup.select_one('.RaceNum')
    if race_num_elem:
        race_data["race_num"] = parse_number(race_num_elem.get_text())
    
    # ãƒ¬ãƒ¼ã‚¹å
    race_name_elem = soup.select_one('.RaceName')
    if race_name_elem:
        race_data["race_name"] = race_name_elem.get_text(strip=True)
    
    # ç«¶é¦¬å ´
    venue_elem = soup.select_one('.RaceData02 span')
    if venue_elem:
        venue_text = venue_elem.get_text(strip=True)
        venue_match = re.search(r'[0-9]+å›(.+?)[0-9]+æ—¥', venue_text)
        if venue_match:
            race_data["venue"] = venue_match.group(1)
        else:
            race_data["venue"] = venue_text[:2] if len(venue_text) >= 2 else venue_text
    
    # ç€é †ãƒ†ãƒ¼ãƒ–ãƒ«
    result_table = soup.select_one('.ResultTableWrap table')
    if result_table:
        rows = result_table.select('tr.HorseList')
        
        for row in rows:
            try:
                rank_elem = row.select_one('.Rank')
                rank = parse_number(rank_elem.get_text()) if rank_elem else 0
                
                umaban_elem = row.select_one('.Umaban')
                umaban = parse_number(umaban_elem.get_text()) if umaban_elem else 0
                
                horse_name_elem = row.select_one('.Horse_Name a')
                horse_name = horse_name_elem.get_text(strip=True) if horse_name_elem else ""
                
                jockey_elem = row.select_one('.Jockey a')
                jockey = jockey_elem.get_text(strip=True) if jockey_elem else ""
                
                time_elem = row.select_one('.Time .RaceTime')
                race_time = time_elem.get_text(strip=True) if time_elem else ""
                
                last3f_elem = row.select_one('.Time .RapTime')
                last3f = last3f_elem.get_text(strip=True) if last3f_elem else ""
                
                odds_elem = row.select_one('.Odds span')
                odds = parse_float(odds_elem.get_text()) if odds_elem else 0.0
                
                horse_result = {
                    "ç€é †": rank,
                    "é¦¬ç•ª": umaban,
                    "é¦¬å": horse_name,
                    "é¨æ‰‹": jockey,
                    "ã‚¿ã‚¤ãƒ ": race_time,
                    "ä¸ŠãŒã‚Š3F": last3f,
                    "ã‚ªãƒƒã‚º": odds
                }
                
                race_data["all_results"].append(horse_result)
                
                if rank <= 3:
                    race_data["top3"].append(horse_result)
                    
            except Exception:
                continue
    
    race_data["top3"] = sorted(race_data["top3"], key=lambda x: x.get("ç€é †", 99))[:3]
    race_data["all_results"] = sorted(race_data["all_results"], key=lambda x: x.get("ç€é †", 99))
    
    # æ‰•æˆ»é‡‘ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    payout_tables = soup.select('.Payout_Detail, .FullWrap .Payout, .PaybackTable')
    
    for table in payout_tables:
        rows = table.select('tr')
        for row in rows:
            try:
                bet_type_elem = row.select_one('.Bet_Type, th')
                if not bet_type_elem:
                    continue
                bet_type = bet_type_elem.get_text(strip=True)
                
                payout_elem = row.select_one('.Payout, .Value, td:nth-child(2)')
                if not payout_elem:
                    continue
                
                payout_value = parse_number(payout_elem.get_text())
                
                bet_type_map = {
                    "å˜å‹": "å˜å‹", "è¤‡å‹": "è¤‡å‹", "æ é€£": "æ é€£",
                    "é¦¬é€£": "é¦¬é€£", "é¦¬å˜": "é¦¬å˜", "ãƒ¯ã‚¤ãƒ‰": "ãƒ¯ã‚¤ãƒ‰",
                    "ä¸‰é€£è¤‡": "ä¸‰é€£è¤‡", "3é€£è¤‡": "ä¸‰é€£è¤‡",
                    "ä¸‰é€£å˜": "ä¸‰é€£å˜", "3é€£å˜": "ä¸‰é€£å˜",
                }
                
                for key, val in bet_type_map.items():
                    if key in bet_type:
                        if val in ["è¤‡å‹", "ãƒ¯ã‚¤ãƒ‰"]:
                            result_elem = row.select_one('.Result, .Num')
                            if result_elem:
                                result_nums = re.findall(r'\d+', result_elem.get_text())
                                if result_nums:
                                    k = "-".join(result_nums) if len(result_nums) > 1 else result_nums[0]
                                    if val not in race_data["payouts"]:
                                        race_data["payouts"][val] = {}
                                    race_data["payouts"][val][k] = payout_value
                        else:
                            race_data["payouts"][val] = payout_value
                        break
            except Exception:
                continue
    
    return race_data


def file_exists_and_valid(filepath: Path) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ã‹ç¢ºèª"""
    if not filepath.exists():
        return False
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            races = data.get("races", [])
            # ãƒ¬ãƒ¼ã‚¹ãŒ1ã¤ä»¥ä¸Šã‚ã‚Šã€ç€é †ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°æœ‰åŠ¹
            if races and len(races) > 0:
                if races[0].get("top3") or races[0].get("all_results"):
                    return True
    except Exception:
        pass
    
    return False


def save_results(results: List[Dict], target_date: datetime, force: bool = False):
    """çµæœãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç®¡ç†ä»˜ãï¼‰"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    
    date_str = target_date.strftime("%Y%m%d")
    filepath = DATA_DIR / f"{RESULTS_PREFIX}{date_str}.json"
    archive_path = ARCHIVE_DIR / f"{RESULTS_PREFIX}{date_str}.json"
    
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆforceã§ãªã„é™ã‚Šï¼‰
    if not force and file_exists_and_valid(filepath):
        print(f"  [SKIP] æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š: {filepath}")
        return False
    
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    if filepath.exists():
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                old_data = json.load(f)
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
            backup_path = ARCHIVE_DIR / f"{RESULTS_PREFIX}{date_str}_backup_{datetime.now().strftime('%Y%m%d%H%M%S')}.json"
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(old_data, f, ensure_ascii=False, indent=2)
            print(f"  [BACKUP] {backup_path}")
        except Exception:
            pass
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    output_data = {
        "date": target_date.strftime("%Y-%m-%d"),
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "races": results
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ã‚‚ã‚³ãƒ”ãƒ¼
    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"  [SAVED] {filepath} ({len(results)}ãƒ¬ãƒ¼ã‚¹)")
    return True


def main():
    print("=" * 60)
    print("ğŸ‡ UMA-Logic Pro - éå»ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # å¼•æ•°ã§å¹´ã‚’æŒ‡å®šå¯èƒ½
    if len(sys.argv) > 1:
        try:
            years = [int(y) for y in sys.argv[1:]]
        except ValueError:
            print("[ERROR] å¹´ã¯æ•°å­—ã§æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹: 2024 2025ï¼‰")
            sys.exit(1)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: éå»2å¹´
        current_year = datetime.now().year
        years = [current_year - 1, current_year]
    
    print(f"[INFO] å¯¾è±¡å¹´: {years}")
    print()
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    
    total_saved = 0
    total_skipped = 0
    
    for year in years:
        print(f"\n{'='*40}")
        print(f"ğŸ“… {year}å¹´ã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
        print(f"{'='*40}")
        
        # é–‹å‚¬æ—¥ãƒªã‚¹ãƒˆã‚’å–å¾—
        race_dates = get_race_dates_for_year(year)
        
        if not race_dates:
            print(f"[WARN] {year}å¹´ã®é–‹å‚¬æ—¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            continue
        
        for i, target_date in enumerate(race_dates):
            date_str = target_date.strftime("%Y-%m-%d")
            filepath = DATA_DIR / f"{RESULTS_PREFIX}{target_date.strftime('%Y%m%d')}.json"
            
            print(f"\n[{i+1}/{len(race_dates)}] {date_str}")
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
            if file_exists_and_valid(filepath):
                print(f"  [SKIP] æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š")
                total_skipped += 1
                continue
            
            # ãƒ¬ãƒ¼ã‚¹IDå–å¾—
            race_ids = get_race_ids_for_date(target_date)
            
            if not race_ids:
                print(f"  [WARN] ãƒ¬ãƒ¼ã‚¹IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            print(f"  [INFO] {len(race_ids)}ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—ä¸­...")
            
            # å„ãƒ¬ãƒ¼ã‚¹ã®çµæœã‚’å–å¾—
            results = []
            for race_id in race_ids:
                result = fetch_race_result(race_id)
                if result:
                    results.append(result)
                time.sleep(REQUEST_INTERVAL)
            
            # ä¿å­˜
            if results:
                if save_results(results, target_date):
                    total_saved += 1
            
            # æ—¥ä»˜é–“ã®å¾…æ©Ÿ
            time.sleep(1)
    
    print("\n" + "=" * 60)
    print(f"âœ… å‡¦ç†å®Œäº†")
    print(f"   æ–°è¦ä¿å­˜: {total_saved}æ—¥åˆ†")
    print(f"   ã‚¹ã‚­ãƒƒãƒ—: {total_skipped}æ—¥åˆ†ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Šï¼‰")
    print("=" * 60)


if __name__ == "__main__":
    main()
