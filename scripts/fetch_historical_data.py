# scripts/fetch_historical_data.py
# UMA-Logic Pro - éå»ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆä¿®æ­£ç‰ˆ ï¼‰
# db.netkeiba.com ã‹ã‚‰éå»ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import sys

# --- å®šæ•° ---
# éå»ãƒ‡ãƒ¼ã‚¿ã¯ db.netkeiba.com ã‚’ä½¿ç”¨
DB_BASE_URL = "https://db.netkeiba.com"
RACE_LIST_URL = "https://db.netkeiba.com/race/list"

DATA_DIR = Path("data" )
ARCHIVE_DIR = DATA_DIR / "archive"
RESULTS_PREFIX = "results_"

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 3
REQUEST_TIMEOUT = 30
REQUEST_INTERVAL = 2.5  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
    "Referer": "https://db.netkeiba.com/",
}


def fetch_with_retry(url: str ) -> Optional[requests.Response]:
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            print(f"  [WARN] ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
    return None


def parse_number(text: str) -> int:
    """æ•°å€¤æŠ½å‡º"""
    if not text:
        return 0
    text = text.replace(',', '').replace('å††', '').replace('Â¥', '')
    nums = re.findall(r'\d+', text)
    return int(nums[0]) if nums else 0


def parse_float(text: str) -> float:
    """å°æ•°æŠ½å‡º"""
    if not text:
        return 0.0
    nums = re.findall(r'[\d.]+', text)
    return float(nums[0]) if nums else 0.0


def get_jra_race_dates(year: int, month: int) -> List[str]:
    """
    æŒ‡å®šå¹´æœˆã®JRAé–‹å‚¬æ—¥ãƒªã‚¹ãƒˆã‚’å–å¾—
    è¿”ã‚Šå€¤: ['20240106', '20240107', ...] å½¢å¼
    """
    # JRAã®é–‹å‚¬ã¯åŸºæœ¬çš„ã«åœŸæ—¥
    # å¹´é–“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰é–‹å‚¬æ—¥ã‚’æ¨å®š
    dates = []
    
    # æœˆã®åˆæ—¥ã‹ã‚‰æœ€çµ‚æ—¥ã¾ã§
    if month == 12:
        next_month = datetime(year + 1, 1, 1)
    else:
        next_month = datetime(year, month + 1, 1)
    
    current = datetime(year, month, 1)
    
    while current < next_month:
        # åœŸæ›œ(5)ã¨æ—¥æ›œ(6)ã‚’é–‹å‚¬æ—¥ã¨ã—ã¦è¿½åŠ 
        if current.weekday() in [5, 6]:
            dates.append(current.strftime("%Y%m%d"))
        current += timedelta(days=1)
    
    return dates


def get_race_ids_from_db(date_str: str) -> List[str]:
    """
    db.netkeiba.com ã‹ã‚‰æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—
    """
    # æ—¥ä»˜ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹IDã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ
    # ãƒ¬ãƒ¼ã‚¹IDå½¢å¼: YYYYJJKKNNRR
    # YYYY: å¹´, JJ: å ´æ‰€ã‚³ãƒ¼ãƒ‰, KK: å›æ¬¡, NN: æ—¥æ¬¡, RR: ãƒ¬ãƒ¼ã‚¹ç•ªå·
    
    # é–‹å‚¬å ´æ‰€ã‚³ãƒ¼ãƒ‰
    venue_codes = {
        "01": "æœ­å¹Œ", "02": "å‡½é¤¨", "03": "ç¦å³¶", "04": "æ–°æ½Ÿ",
        "05": "æ±äº¬", "06": "ä¸­å±±", "07": "ä¸­äº¬", "08": "äº¬éƒ½",
        "09": "é˜ªç¥", "10": "å°å€‰"
    }
    
    race_ids = []
    year = date_str[:4]
    
    # å„ç«¶é¦¬å ´ã‚’ãƒã‚§ãƒƒã‚¯
    for venue_code in venue_codes.keys():
        # å›æ¬¡ã¯1ã€œ5ç¨‹åº¦ã€æ—¥æ¬¡ã¯1ã€œ12ç¨‹åº¦
        for kai in range(1, 6):
            for nichi in range(1, 13):
                # 12ãƒ¬ãƒ¼ã‚¹åˆ†ã®IDã‚’ç”Ÿæˆ
                for race_num in range(1, 13):
                    race_id = f"{year}{venue_code}{kai:02d}{nichi:02d}{race_num:02d}"
                    race_ids.append(race_id)
    
    return race_ids


def fetch_race_result_from_db(race_id: str) -> Optional[Dict]:
    """
    db.netkeiba.com ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—
    """
    url = f"{DB_BASE_URL}/race/{race_id}/"
    
    response = fetch_with_retry(url)
    if not response:
        return None
    
    # æ–‡å­—ã‚³ãƒ¼ãƒ‰å‡¦ç†
    try:
        html = response.content.decode('euc-jp', errors='replace')
    except:
        html = response.text
    
    soup = BeautifulSoup(html, 'lxml')
    
    # ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    title = soup.find('title')
    if not title or 'ãƒ¬ãƒ¼ã‚¹çµæœ' not in title.get_text():
        return None
    
    race_data = {
        "race_id": race_id,
        "race_num": 0,
        "race_name": "",
        "venue": "",
        "top3": [],
        "all_results": [],
        "payouts": {}
    }
    
    # ãƒ¬ãƒ¼ã‚¹æƒ…å ±
    race_name_elem = soup.select_one('.racedata fc h1, .data_intro h1, h1')
    if race_name_elem:
        race_data["race_name"] = race_name_elem.get_text(strip=True)
    
    # ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’æŠ½å‡º
    race_num_match = re.search(r'(\d+)R', race_data.get("race_name", ""))
    if race_num_match:
        race_data["race_num"] = int(race_num_match.group(1))
    else:
        # ãƒ¬ãƒ¼ã‚¹IDã‹ã‚‰æŠ½å‡º
        race_data["race_num"] = int(race_id[-2:])
    
    # ç«¶é¦¬å ´
    venue_code = race_id[4:6]
    venue_map = {
        "01": "æœ­å¹Œ", "02": "å‡½é¤¨", "03": "ç¦å³¶", "04": "æ–°æ½Ÿ",
        "05": "æ±äº¬", "06": "ä¸­å±±", "07": "ä¸­äº¬", "08": "äº¬éƒ½",
        "09": "é˜ªç¥", "10": "å°å€‰"
    }
    race_data["venue"] = venue_map.get(venue_code, "ä¸æ˜")
    
    # ç€é †ãƒ†ãƒ¼ãƒ–ãƒ«
    result_table = soup.select_one('.race_table_01, table.nk_tb_common')
    if result_table:
        rows = result_table.select('tr')[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
        
        for row in rows:
            cells = row.select('td')
            if len(cells) < 10:
                continue
            
            try:
                # ç€é †
                rank_text = cells[0].get_text(strip=True)
                rank = parse_number(rank_text)
                if rank == 0:
                    continue
                
                # é¦¬ç•ª
                umaban = parse_number(cells[2].get_text(strip=True))
                
                # é¦¬å
                horse_name_elem = cells[3].select_one('a')
                horse_name = horse_name_elem.get_text(strip=True) if horse_name_elem else cells[3].get_text(strip=True)
                
                # é¨æ‰‹
                jockey_elem = cells[6].select_one('a')
                jockey = jockey_elem.get_text(strip=True) if jockey_elem else cells[6].get_text(strip=True)
                
                # ã‚¿ã‚¤ãƒ 
                race_time = cells[7].get_text(strip=True) if len(cells) > 7 else ""
                
                # ä¸ŠãŒã‚Š3F
                last3f = ""
                if len(cells) > 11:
                    last3f = cells[11].get_text(strip=True)
                
                # ã‚ªãƒƒã‚º
                odds = 0.0
                if len(cells) > 12:
                    odds = parse_float(cells[12].get_text(strip=True))
                elif len(cells) > 10:
                    odds = parse_float(cells[10].get_text(strip=True))
                
                horse_result = {
                    "ç€é †": rank,
                    "é¦¬ç•ª": umaban,
                    "é¦¬å": horse_name,
                    "é¨æ‰‹": jockey,
                    "ã‚¿ã‚¤ãƒ ": race_time,
                    "ä¸ŠãŒã‚Š3F": last3f,
                    "ã‚ªãƒƒã‚º": odds
                }
                
                race_data["all_results"].append(horse_result)
                
                if rank <= 3:
                    race_data["top3"].append(horse_result)
                    
            except Exception as e:
                continue
    
    # ãƒ‡ãƒ¼ã‚¿ãŒãªã‘ã‚Œã°None
    if not race_data["all_results"]:
        return None
    
    race_data["top3"] = sorted(race_data["top3"], key=lambda x: x.get("ç€é †", 99))[:3]
    race_data["all_results"] = sorted(race_data["all_results"], key=lambda x: x.get("ç€é †", 99))
    
    # æ‰•æˆ»é‡‘ãƒ†ãƒ¼ãƒ–ãƒ«
    payout_tables = soup.select('.pay_table_01, .pay_block table')
    
    for table in payout_tables:
        rows = table.select('tr')
        for row in rows:
            header = row.select_one('th')
            value_cell = row.select_one('td')
            
            if not header or not value_cell:
                continue
            
            bet_type = header.get_text(strip=True)
            
            # æ‰•æˆ»é‡‘é¡ã‚’å–å¾—
            payout_text = value_cell.get_text(strip=True)
            payout_value = parse_number(payout_text)
            
            if "å˜å‹" in bet_type:
                race_data["payouts"]["å˜å‹"] = payout_value
            elif "è¤‡å‹" in bet_type:
                # è¤‡å‹ã¯è¤‡æ•°ã‚ã‚‹å ´åˆãŒã‚ã‚‹
                if "è¤‡å‹" not in race_data["payouts"]:
                    race_data["payouts"]["è¤‡å‹"] = {}
                nums = re.findall(r'(\d+)\s*[\-ï¼]\s*(\d+)', payout_text)
                if nums:
                    for num, pay in nums:
                        race_data["payouts"]["è¤‡å‹"][num] = parse_number(pay)
                else:
                    race_data["payouts"]["è¤‡å‹"]["1"] = payout_value
            elif "æ é€£" in bet_type:
                race_data["payouts"]["æ é€£"] = payout_value
            elif "é¦¬é€£" in bet_type:
                race_data["payouts"]["é¦¬é€£"] = payout_value
            elif "é¦¬å˜" in bet_type:
                race_data["payouts"]["é¦¬å˜"] = payout_value
            elif "ãƒ¯ã‚¤ãƒ‰" in bet_type:
                if "ãƒ¯ã‚¤ãƒ‰" not in race_data["payouts"]:
                    race_data["payouts"]["ãƒ¯ã‚¤ãƒ‰"] = {}
                race_data["payouts"]["ãƒ¯ã‚¤ãƒ‰"]["1"] = payout_value
            elif "ä¸‰é€£è¤‡" in bet_type or "3é€£è¤‡" in bet_type:
                race_data["payouts"]["ä¸‰é€£è¤‡"] = payout_value
            elif "ä¸‰é€£å˜" in bet_type or "3é€£å˜" in bet_type:
                race_data["payouts"]["ä¸‰é€£å˜"] = payout_value
    
    return race_data


def file_exists_and_valid(filepath: Path) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ã‹ç¢ºèª"""
    if not filepath.exists():
        return False
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            races = data.get("races", [])
            if races and len(races) > 0:
                if races[0].get("top3") or races[0].get("all_results"):
                    return True
    except:
        pass
    
    return False


def save_results(results: List[Dict], target_date: datetime):
    """çµæœãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    
    date_str = target_date.strftime("%Y%m%d")
    filepath = DATA_DIR / f"{RESULTS_PREFIX}{date_str}.json"
    archive_path = ARCHIVE_DIR / f"{RESULTS_PREFIX}{date_str}.json"
    
    output_data = {
        "date": target_date.strftime("%Y-%m-%d"),
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "races": results
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"  [SAVED] {filepath} ({len(results)}ãƒ¬ãƒ¼ã‚¹)")


def fetch_date_results(date_str: str) -> List[Dict]:
    """
    æŒ‡å®šæ—¥ã®å…¨ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—
    """
    year = date_str[:4]
    
    # å„ç«¶é¦¬å ´ãƒ»å›æ¬¡ãƒ»æ—¥æ¬¡ã‚’è©¦ã™
    venue_codes = ["01", "02", "03", "04", "05", "06", "07", "08", "09", "10"]
    
    results = []
    found_venues = set()
    
    for venue_code in venue_codes:
        # å›æ¬¡ã¨æ—¥æ¬¡ã‚’æ¨å®šã—ã¦è©¦ã™
        for kai in range(1, 6):
            for nichi in range(1, 13):
                venue_found = False
                
                for race_num in range(1, 13):
                    race_id = f"{year}{venue_code}{kai:02d}{nichi:02d}{race_num:02d}"
                    
                    # æ—¢ã«è¦‹ã¤ã‹ã£ãŸç«¶é¦¬å ´ã®å ´åˆã¯ç¶šã‘ã‚‹
                    if venue_code in found_venues and race_num > 1:
                        result = fetch_race_result_from_db(race_id)
                        if result:
                            results.append(result)
                            time.sleep(REQUEST_INTERVAL)
                        continue
                    
                    # 1Rã‚’è©¦ã—ã¦ã“ã®é–‹å‚¬ãŒã‚ã‚‹ã‹ç¢ºèª
                    if race_num == 1:
                        result = fetch_race_result_from_db(race_id)
                        if result:
                            results.append(result)
                            found_venues.add(venue_code)
                            venue_found = True
                            time.sleep(REQUEST_INTERVAL)
                        else:
                            break  # ã“ã®å›æ¬¡ãƒ»æ—¥æ¬¡ã¯å­˜åœ¨ã—ãªã„
                    elif venue_found:
                        result = fetch_race_result_from_db(race_id)
                        if result:
                            results.append(result)
                            time.sleep(REQUEST_INTERVAL)
                
                if not venue_found:
                    break  # ã“ã®å›æ¬¡ã¯å­˜åœ¨ã—ãªã„
    
    return results


def main():
    print("=" * 60)
    print("ğŸ‡ UMA-Logic Pro - éå»ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    print("=" * 60)
    
    # å¼•æ•°ã§å¹´ã‚’æŒ‡å®šå¯èƒ½
    if len(sys.argv) > 1:
        try:
            years = [int(y) for y in sys.argv[1:]]
        except ValueError:
            print("[ERROR] å¹´ã¯æ•°å­—ã§æŒ‡å®šã—ã¦ãã ã•ã„")
            sys.exit(1)
    else:
        current_year = datetime.now().year
        years = [current_year - 1, current_year]
    
    print(f"[INFO] å¯¾è±¡å¹´: {years}")
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    
    total_saved = 0
    total_skipped = 0
    
    for year in years:
        print(f"\n{'='*40}")
        print(f"ğŸ“… {year}å¹´ã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
        print(f"{'='*40}")
        
        for month in range(1, 13):
            print(f"\n[INFO] {year}å¹´{month}æœˆ")
            
            # åœŸæ—¥ã®æ—¥ä»˜ãƒªã‚¹ãƒˆã‚’å–å¾—
            dates = get_jra_race_dates(year, month)
            
            for date_str in dates:
                target_date = datetime.strptime(date_str, "%Y%m%d")
                filepath = DATA_DIR / f"{RESULTS_PREFIX}{date_str}.json"
                
                # æœªæ¥ã®æ—¥ä»˜ã¯ã‚¹ã‚­ãƒƒãƒ—
                if target_date > datetime.now():
                    continue
                
                print(f"\n  [{date_str}] {target_date.strftime('%m/%d')}")
                
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
                if file_exists_and_valid(filepath):
                    print(f"    [SKIP] æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š")
                    total_skipped += 1
                    continue
                
                # ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—
                results = fetch_date_results(date_str)
                
                if results:
                    save_results(results, target_date)
                    total_saved += 1
                else:
                    print(f"    [WARN] ãƒ‡ãƒ¼ã‚¿ãªã—")
    
    print("\n" + "=" * 60)
    print(f"âœ… å‡¦ç†å®Œäº†")
    print(f"   æ–°è¦ä¿å­˜: {total_saved}æ—¥åˆ†")
    print(f"   ã‚¹ã‚­ãƒƒãƒ—: {total_skipped}æ—¥åˆ†")
    print("=" * 60)


if __name__ == "__main__":
    main()
