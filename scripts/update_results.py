# scripts/update_results.py
# UMA-Logic Pro - ãƒ¬ãƒ¼ã‚¹çµæœå–å¾—ãƒ»çš„ä¸­åˆ¤å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# æ¨å¥¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ results_YYYYMMDD.json ã‚’ä¿å­˜

import requests
from bs4 import BeautifulSoup
import json
import os
import time
import re
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# pytzãŒãªã„ç’°å¢ƒã§ã‚‚å‹•ä½œ
try:
    import pytz
    JST = pytz.timezone('Asia/Tokyo')
except ImportError:
    JST = None

# --- å®šæ•° ---
BASE_URL = "https://race.netkeiba.com"
RESULT_URL = "https://race.netkeiba.com/race/result.html"
RACE_LIST_URL = "https://race.netkeiba.com/top/race_list.html"

DATA_DIR = Path("data")
PREDICTIONS_PREFIX = "predictions_"
RESULTS_PREFIX = "results_"
HISTORY_FILE = "history.json"

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 2
REQUEST_TIMEOUT = 15
REQUEST_INTERVAL = 1.5

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
}

# ä¸­å¤®ç«¶é¦¬ã®ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
VENUE_CODES = {
    "01": "æœ­å¹Œ",
    "02": "å‡½é¤¨",
    "03": "ç¦å³¶",
    "04": "æ–°æ½Ÿ",
    "05": "æ±äº¬",
    "06": "ä¸­å±±",
    "07": "ä¸­äº¬",
    "08": "äº¬éƒ½",
    "09": "é˜ªç¥",
    "10": "å°å€‰"
}


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---

def get_jst_now():
    """æ—¥æœ¬æ™‚é–“ã®ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—ï¼ˆç¢ºå®Ÿã«JSTã§å–å¾—ï¼‰"""
    try:
        import pytz
        JST = pytz.timezone('Asia/Tokyo')
        utc_now = datetime.now(timezone.utc)
        return utc_now.astimezone(JST)
    except ImportError:
        jst = timezone(timedelta(hours=9))
        utc_now = datetime.now(timezone.utc)
        return utc_now.astimezone(jst)


def fetch_with_retry(url: str, params: dict = None) -> Optional[requests.Response]:
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(
                url,
                params=params,
                headers=HEADERS,
                timeout=REQUEST_TIMEOUT
            )
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            print(f"[WARN] ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
    return None


def detect_encoding(response: requests.Response) -> str:
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’æ¤œå‡º"""
    if response.encoding:
        return response.encoding
    content = response.content[:1000].lower()
    if b'euc-jp' in content:
        return 'euc-jp'
    elif b'shift_jis' in content or b'sjis' in content:
        return 'shift_jis'
    return 'utf-8'


def parse_number(text: str) -> int:
    """æ–‡å­—åˆ—ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
    if not text:
        return 0
    nums = re.findall(r'[\d,]+', text.replace(',', ''))
    return int(nums[0]) if nums else 0


def parse_float(text: str) -> float:
    """æ–‡å­—åˆ—ã‹ã‚‰å°æ•°ã‚’æŠ½å‡º"""
    if not text:
        return 0.0
    nums = re.findall(r'[\d.]+', text)
    return float(nums[0]) if nums else 0.0


# --- ãƒ¬ãƒ¼ã‚¹IDç”Ÿæˆãƒ»æ¤œç´¢ï¼ˆæ–°æ–¹å¼ï¼‰ ---

def get_likely_kaisai_codes(target_date: datetime) -> List[str]:
    """
    æ—¥ä»˜ã‹ã‚‰é–‹å‚¬ã‚³ãƒ¼ãƒ‰ã‚’æ¨æ¸¬
    
    é–‹å‚¬ã‚³ãƒ¼ãƒ‰ã¯å¹´åˆã‹ã‚‰ã®é–‹å‚¬é€±ã§ã‚«ã‚¦ãƒ³ãƒˆã•ã‚Œã‚‹
    ä¾‹: 1æœˆç¬¬1é€±=01, 1æœˆç¬¬2é€±=02, ...
    """
    month = target_date.month
    
    if month <= 2:
        return ["01", "02", "03", "04", "05"]
    elif month <= 4:
        return ["03", "04", "05", "06", "07", "08"]
    elif month <= 6:
        return ["06", "07", "08", "09", "10", "11"]
    elif month <= 8:
        return ["09", "10", "11", "12", "13", "14"]
    elif month <= 10:
        return ["12", "13", "14", "15", "16", "17"]
    else:
        return ["15", "16", "17", "18", "19", "20"]


def generate_possible_race_ids(target_date: datetime) -> List[str]:
    """
    æŒ‡å®šæ—¥ã®å…¨ã¦ã®å¯èƒ½æ€§ã®ã‚ã‚‹race_idã‚’ç”Ÿæˆã™ã‚‹
    
    ä¸­å¤®ç«¶é¦¬ã®race_idå½¢å¼: 2026XXYYZZMM
    - 2026: å¹´
    - XX: é–‹å‚¬ã‚³ãƒ¼ãƒ‰ (01-20ç¨‹åº¦ã€é–‹å‚¬é€±ã«ã‚ˆã‚‹)
    - YY: ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ (01-10)
    - ZZ: æ—¥ä»˜ã®ä¸‹2æ¡
    - MM: ãƒ¬ãƒ¼ã‚¹ç•ªå· (01-12)
    """
    year = target_date.year
    date_2digit = target_date.strftime("%d")
    
    # é–‹å‚¬ã‚³ãƒ¼ãƒ‰ã‚’æ¨æ¸¬
    kaisai_codes = get_likely_kaisai_codes(target_date)
    
    race_ids = []
    
    # å…¨ç«¶é¦¬å ´Ã—å…¨é–‹å‚¬ã‚³ãƒ¼ãƒ‰Ã—å…¨ãƒ¬ãƒ¼ã‚¹ç•ªå·ã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
    for kaisai_code in kaisai_codes:
        for venue_code in VENUE_CODES.keys():
            for race_num in range(1, 13):  # 1Rï½12R
                race_id = f"{year}{kaisai_code}{venue_code}{date_2digit}{race_num:02d}"
                race_ids.append(race_id)
    
    return race_ids


def check_race_exists(race_id: str) -> bool:
    """
    æŒ‡å®šã—ãŸrace_idã®ãƒ¬ãƒ¼ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    """
    url = f"{RESULT_URL}?race_id={race_id}"
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=5)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰200 ã‹ã¤ ãƒ¬ãƒ¼ã‚¹çµæœãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
        if response.status_code == 200:
            content = response.text
            # 404ãƒšãƒ¼ã‚¸ã‚„ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if "ResultTableWrap" in content or "ç€é †" in content:
                return True
        
        return False
        
    except Exception:
        return False


def get_race_ids_for_date_v2(target_date: datetime) -> List[str]:
    """
    æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ”¹å–„ç‰ˆãƒ»ä¸¦åˆ—å‡¦ç†ï¼‰
    
    ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
    1. å¯èƒ½æ€§ã®ã‚ã‚‹å…¨race_idã‚’ç”Ÿæˆ
    2. å„IDã«å¯¾ã—ã¦çµæœãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹ã‹ä¸¦åˆ—ç¢ºèª
    3. å­˜åœ¨ã™ã‚‹race_idã®ã¿ã‚’è¿”ã™
    """
    print(f"[INFO] ãƒ¬ãƒ¼ã‚¹IDæ¢ç´¢é–‹å§‹: {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
    
    # å¯èƒ½æ€§ã®ã‚ã‚‹å…¨race_idã‚’ç”Ÿæˆ
    possible_ids = generate_possible_race_ids(target_date)
    
    print(f"[INFO] {len(possible_ids)}å€‹ã®å€™è£œã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
    
    valid_race_ids = []
    
    # ä¸¦åˆ—å‡¦ç†ã§é«˜é€ŸåŒ–
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_id = {
            executor.submit(check_race_exists, race_id): race_id 
            for race_id in possible_ids
        }
        
        for future in as_completed(future_to_id):
            race_id = future_to_id[future]
            try:
                if future.result():
                    valid_race_ids.append(race_id)
                    # ç«¶é¦¬å ´åã‚’å–å¾—
                    venue_code = race_id[6:8]
                    venue_name = VENUE_CODES.get(venue_code, "ä¸æ˜")
                    race_num = int(race_id[-2:])
                    print(f"  âœ“ ãƒ¬ãƒ¼ã‚¹ç™ºè¦‹: {race_id} ({venue_name}{race_num}R)")
            except Exception as e:
                pass  # ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ
    
    # race_idã§ã‚½ãƒ¼ãƒˆ
    valid_race_ids = sorted(valid_race_ids)
    
    print(f"[INFO] {len(valid_race_ids)}ä»¶ã®ãƒ¬ãƒ¼ã‚¹ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
    
    return valid_race_ids


def get_race_ids_for_date(target_date: datetime) -> List[str]:
    """
    æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ—§æ–¹å¼ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
    """
    date_str = target_date.strftime("%Y%m%d")
    url = f"{RACE_LIST_URL}?kaisai_date={date_str}"
    
    print(f"[INFO] ãƒ¬ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—ä¸­ï¼ˆæ—§æ–¹å¼ï¼‰: {url}")
    
    response = fetch_with_retry(url)
    if not response:
        print("[ERROR] ãƒ¬ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return []
    
    encoding = detect_encoding(response)
    soup = BeautifulSoup(response.content.decode(encoding, errors='replace'), 'lxml')
    
    race_ids = []
    
    # ãƒ¬ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‹ã‚‰IDã‚’æŠ½å‡º
    for link in soup.find_all('a', href=True):
        href = link['href']
        if 'race_id=' in href:
            match = re.search(r'race_id=(\d+)', href)
            if match:
                race_id = match.group(1)
                if race_id not in race_ids:
                    race_ids.append(race_id)
    
    print(f"[INFO] {len(race_ids)}ä»¶ã®ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ã—ã¾ã—ãŸ")
    return race_ids


# --- ãƒ¬ãƒ¼ã‚¹çµæœå–å¾— ---

def fetch_race_result(race_id: str) -> Optional[Dict]:
    """
    ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—ã—ã€æ¨å¥¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§è¿”ã™
    """
    url = f"{RESULT_URL}?race_id={race_id}"
    
    print(f"[INFO] çµæœå–å¾—ä¸­: {race_id}")
    
    response = fetch_with_retry(url)
    if not response:
        return None
    
    encoding = detect_encoding(response)
    soup = BeautifulSoup(response.content.decode(encoding, errors='replace'), 'lxml')
    
    # --- ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ± ---
    race_data = {
        "race_id": race_id,
        "race_num": 0,
        "race_name": "",
        "venue": "",
        "top3": [],
        "all_results": [],
        "payouts": {}
    }
    
    # ãƒ¬ãƒ¼ã‚¹ç•ªå·
    race_num_elem = soup.select_one('.RaceNum')
    if race_num_elem:
        race_data["race_num"] = parse_number(race_num_elem.get_text())
    
    # ãƒ¬ãƒ¼ã‚¹å
    race_name_elem = soup.select_one('.RaceName')
    if race_name_elem:
        race_data["race_name"] = race_name_elem.get_text(strip=True)
    
    # ç«¶é¦¬å ´
    venue_elem = soup.select_one('.RaceData02 span')
    if venue_elem:
        venue_text = venue_elem.get_text(strip=True)
        # ã€Œ1å›æ±äº¬1æ—¥ã€â†’ã€Œæ±äº¬ã€
        venue_match = re.search(r'[0-9]+å›(.+?)[0-9]+æ—¥', venue_text)
        if venue_match:
            race_data["venue"] = venue_match.group(1)
        else:
            race_data["venue"] = venue_text[:2] if len(venue_text) >= 2 else venue_text
    
    # race_idã‹ã‚‰ç«¶é¦¬å ´ã‚’æ¨æ¸¬ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if not race_data["venue"]:
        venue_code = race_id[6:8]
        race_data["venue"] = VENUE_CODES.get(venue_code, "ä¸æ˜")
    
    # --- ç€é †ãƒ†ãƒ¼ãƒ–ãƒ« ---
    result_table = soup.select_one('.ResultTableWrap table')
    if result_table:
        rows = result_table.select('tr.HorseList')
        
        for row in rows:
            try:
                # ç€é †
                rank_elem = row.select_one('.Rank')
                rank = parse_number(rank_elem.get_text()) if rank_elem else 0
                
                # é¦¬ç•ª
                umaban_elem = row.select_one('.Umaban')
                umaban = parse_number(umaban_elem.get_text()) if umaban_elem else 0
                
                # é¦¬å
                horse_name_elem = row.select_one('.Horse_Name a')
                horse_name = horse_name_elem.get_text(strip=True) if horse_name_elem else ""
                
                # é¨æ‰‹
                jockey_elem = row.select_one('.Jockey a')
                jockey = jockey_elem.get_text(strip=True) if jockey_elem else ""
                
                # ã‚¿ã‚¤ãƒ 
                time_elem = row.select_one('.Time .RaceTime')
                race_time = time_elem.get_text(strip=True) if time_elem else ""
                
                # ä¸ŠãŒã‚Š3F
                last3f_elem = row.select_one('.Time .RapTime')
                last3f = last3f_elem.get_text(strip=True) if last3f_elem else ""
                
                # å˜å‹ã‚ªãƒƒã‚º
                odds_elem = row.select_one('.Odds span')
                odds = parse_float(odds_elem.get_text()) if odds_elem else 0.0
                
                horse_result = {
                    "ç€é †": rank,
                    "é¦¬ç•ª": umaban,
                    "é¦¬å": horse_name,
                    "é¨æ‰‹": jockey,
                    "ã‚¿ã‚¤ãƒ ": race_time,
                    "ä¸ŠãŒã‚Š3F": last3f,
                    "ã‚ªãƒƒã‚º": odds
                }
                
                race_data["all_results"].append(horse_result)
                
                # ä¸Šä½3é ­ã‚’top3ã«è¿½åŠ 
                if rank <= 3:
                    race_data["top3"].append(horse_result)
                    
            except Exception as e:
                print(f"[WARN] ç€é †ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
    
    # top3ã‚’ç€é †ã§ã‚½ãƒ¼ãƒˆ
    race_data["top3"] = sorted(race_data["top3"], key=lambda x: x.get("ç€é †", 99))[:3]
    race_data["all_results"] = sorted(race_data["all_results"], key=lambda x: x.get("ç€é †", 99))
    
    # --- æ‰•æˆ»é‡‘ãƒ†ãƒ¼ãƒ–ãƒ« ---
    payout_tables = soup.select('.Payout_Detail, .FullWrap .Payout')
    
    for table in payout_tables:
        rows = table.select('tr')
        
        for row in rows:
            try:
                # åˆ¸ç¨®å
                bet_type_elem = row.select_one('.Bet_Type, th')
                if not bet_type_elem:
                    continue
                bet_type = bet_type_elem.get_text(strip=True)
                
                # æ‰•æˆ»é‡‘
                payout_elem = row.select_one('.Payout, .Value')
                if not payout_elem:
                    continue
                
                payout_text = payout_elem.get_text(strip=True)
                payout_value = parse_number(payout_text)
                
                # åˆ¸ç¨®ã‚’æ­£è¦åŒ–
                bet_type_map = {
                    "å˜å‹": "å˜å‹",
                    "è¤‡å‹": "è¤‡å‹",
                    "æ é€£": "æ é€£",
                    "é¦¬é€£": "é¦¬é€£",
                    "é¦¬å˜": "é¦¬å˜",
                    "ãƒ¯ã‚¤ãƒ‰": "ãƒ¯ã‚¤ãƒ‰",
                    "ä¸‰é€£è¤‡": "ä¸‰é€£è¤‡",
                    "3é€£è¤‡": "ä¸‰é€£è¤‡",
                    "ä¸‰é€£å˜": "ä¸‰é€£å˜",
                    "3é€£å˜": "ä¸‰é€£å˜",
                }
                
                normalized_type = None
                for key, val in bet_type_map.items():
                    if key in bet_type:
                        normalized_type = val
                        break
                
                if normalized_type:
                    # è¤‡å‹ãƒ»ãƒ¯ã‚¤ãƒ‰ã¯è¤‡æ•°ã®æ‰•æˆ»ãŒã‚ã‚‹å ´åˆãŒã‚ã‚‹
                    if normalized_type in ["è¤‡å‹", "ãƒ¯ã‚¤ãƒ‰"]:
                        # é¦¬ç•ªã‚’å–å¾—
                        result_elem = row.select_one('.Result, .Num')
                        if result_elem:
                            result_nums = re.findall(r'\d+', result_elem.get_text())
                            if result_nums:
                                key = "-".join(result_nums) if len(result_nums) > 1 else result_nums[0]
                                if normalized_type not in race_data["payouts"]:
                                    race_data["payouts"][normalized_type] = {}
                                race_data["payouts"][normalized_type][key] = payout_value
                    else:
                        race_data["payouts"][normalized_type] = payout_value
                        
            except Exception as e:
                print(f"[WARN] æ‰•æˆ»é‡‘è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
    
    # æ‰•æˆ»é‡‘ã®åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æï¼ˆnetkeibaå½¢å¼ï¼‰
    if not race_data["payouts"]:
        payout_block = soup.select_one('#All_Result_PayBack, .PaybackTable')
        if payout_block:
            # å˜å‹
            tansho = payout_block.select_one('.Tansho .Value, [class*="Tansho"] .Payout')
            if tansho:
                race_data["payouts"]["å˜å‹"] = parse_number(tansho.get_text())
            
            # è¤‡å‹
            fukusho_rows = payout_block.select('.Fukusho tr, [class*="Fukusho"]')
            if fukusho_rows:
                fukusho_dict = {}
                for fr in fukusho_rows:
                    num_elem = fr.select_one('.Num, .Result')
                    val_elem = fr.select_one('.Value, .Payout')
                    if num_elem and val_elem:
                        num = num_elem.get_text(strip=True)
                        val = parse_number(val_elem.get_text())
                        if num and val:
                            fukusho_dict[num] = val
                if fukusho_dict:
                    race_data["payouts"]["è¤‡å‹"] = fukusho_dict
            
            # é¦¬é€£
            umaren = payout_block.select_one('.Umaren .Value, [class*="Umaren"] .Payout')
            if umaren:
                race_data["payouts"]["é¦¬é€£"] = parse_number(umaren.get_text())
            
            # é¦¬å˜
            umatan = payout_block.select_one('.Umatan .Value, [class*="Umatan"] .Payout')
            if umatan:
                race_data["payouts"]["é¦¬å˜"] = parse_number(umatan.get_text())
            
            # ãƒ¯ã‚¤ãƒ‰
            wide_rows = payout_block.select('.Wide tr, [class*="Wide"]')
            if wide_rows:
                wide_dict = {}
                for wr in wide_rows:
                    num_elem = wr.select_one('.Num, .Result')
                    val_elem = wr.select_one('.Value, .Payout')
                    if num_elem and val_elem:
                        num = num_elem.get_text(strip=True).replace(' ', '-').replace('ã€€', '-')
                        val = parse_number(val_elem.get_text())
                        if num and val:
                            wide_dict[num] = val
                if wide_dict:
                    race_data["payouts"]["ãƒ¯ã‚¤ãƒ‰"] = wide_dict
            
            # ä¸‰é€£è¤‡
            sanrenpuku = payout_block.select_one('.Sanrenpuku .Value, [class*="Sanrenpuku"] .Payout')
            if sanrenpuku:
                race_data["payouts"]["ä¸‰é€£è¤‡"] = parse_number(sanrenpuku.get_text())
            
            # ä¸‰é€£å˜
            sanrentan = payout_block.select_one('.Sanrentan .Value, [class*="Sanrentan"] .Payout')
            if sanrentan:
                race_data["payouts"]["ä¸‰é€£å˜"] = parse_number(sanrentan.get_text())
    
    return race_data


# --- çš„ä¸­åˆ¤å®š ---

def check_hit(prediction: Dict, result: Dict) -> Dict:
    """äºˆæƒ³ã¨çµæœã‚’ç…§åˆã—ã¦çš„ä¸­åˆ¤å®š"""
    hit_result = {
        "å˜å‹": {"hit": False, "payout": 0},
        "è¤‡å‹": {"hit": False, "payout": 0},
        "é¦¬é€£": {"hit": False, "payout": 0},
        "ä¸‰é€£è¤‡": {"hit": False, "payout": 0},
    }
    
    if not result or not prediction:
        return hit_result
    
    top3 = result.get("top3", [])
    if len(top3) < 3:
        return hit_result
    
    first = top3[0].get("é¦¬ç•ª", 0)
    second = top3[1].get("é¦¬ç•ª", 0)
    third = top3[2].get("é¦¬ç•ª", 0)
    
    horses = prediction.get("horses", [])
    honmei = next((h["é¦¬ç•ª"] for h in horses if h.get("å°") == "â—"), 0)
    taikou = next((h["é¦¬ç•ª"] for h in horses if h.get("å°") == "â—‹"), 0)
    tanpana = next((h["é¦¬ç•ª"] for h in horses if h.get("å°") == "â–²"), 0)
    
    payouts = result.get("payouts", {})
    
    # å˜å‹
    if honmei == first:
        hit_result["å˜å‹"] = {"hit": True, "payout": payouts.get("å˜å‹", 0)}
    
    # è¤‡å‹
    if honmei in [first, second, third]:
        fukusho = payouts.get("è¤‡å‹", {})
        payout = fukusho.get(str(honmei), 0) if isinstance(fukusho, dict) else 0
        hit_result["è¤‡å‹"] = {"hit": True, "payout": payout}
    
    # é¦¬é€£
    if {honmei, taikou} == {first, second}:
        hit_result["é¦¬é€£"] = {"hit": True, "payout": payouts.get("é¦¬é€£", 0)}
    
    # ä¸‰é€£è¤‡
    if {honmei, taikou, tanpana} == {first, second, third}:
        hit_result["ä¸‰é€£è¤‡"] = {"hit": True, "payout": payouts.get("ä¸‰é€£è¤‡", 0)}
    
    return hit_result


# --- å±¥æ­´æ›´æ–° ---

def load_history() -> List[Dict]:
    """çš„ä¸­å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
    history_path = DATA_DIR / HISTORY_FILE
    if history_path.exists():
        try:
            with open(history_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            pass
    return []


def save_history(history: List[Dict]):
    """çš„ä¸­å±¥æ­´ã‚’ä¿å­˜"""
    history_path = DATA_DIR / HISTORY_FILE
    with open(history_path, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    print(f"[INFO] å±¥æ­´ä¿å­˜å®Œäº†: {history_path}")


def update_history(prediction: Dict, result: Dict, history: List[Dict]):
    """çš„ä¸­ã—ãŸå ´åˆã€å±¥æ­´ã«è¿½åŠ """
    hit_info = check_hit(prediction, result)
    
    for bet_type, info in hit_info.items():
        if info["hit"] and info["payout"] > 0:
            entry = {
                "æ—¥ä»˜": prediction.get("date", ""),
                "ä¼šå ´": prediction.get("venue", result.get("venue", "")),
                "R": prediction.get("race_num", result.get("race_num", 0)),
                "ãƒ¬ãƒ¼ã‚¹å": result.get("race_name", ""),
                "åˆ¸ç¨®": bet_type,
                "çš„ä¸­é…å½“é‡‘": info["payout"],
                "æŠ•è³‡é¡": 100,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæŠ•è³‡é¡
                "æœ¬å‘½é¦¬": next((h.get("é¦¬å", "") for h in prediction.get("horses", []) if h.get("å°") == "â—"), ""),
                "è¨˜éŒ²æ—¥æ™‚": get_jst_now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            is_duplicate = any(
                h.get("æ—¥ä»˜") == entry["æ—¥ä»˜"] and
                h.get("ä¼šå ´") == entry["ä¼šå ´"] and
                h.get("R") == entry["R"] and
                h.get("åˆ¸ç¨®") == entry["åˆ¸ç¨®"]
                for h in history
            )
            
            if not is_duplicate:
                history.append(entry)
                print(f"[HIT] ğŸ¯ {entry['ä¼šå ´']}{entry['R']}R {bet_type} Â¥{info['payout']:,}")


# --- ãƒ‡ãƒ¼ã‚¿ä¿å­˜ ---

def save_results(results: List[Dict], target_date: datetime):
    """çµæœãƒ‡ãƒ¼ã‚¿ã‚’æ¨å¥¨æ§‹é€ ã§ä¿å­˜"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    date_str = target_date.strftime("%Y%m%d")
    filepath = DATA_DIR / f"{RESULTS_PREFIX}{date_str}.json"
    
    output_data = {
        "date": target_date.strftime("%Y-%m-%d"),
        "updated_at": get_jst_now().strftime("%Y-%m-%d %H:%M:%S"),
        "races": results
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"[INFO] çµæœä¿å­˜å®Œäº†: {filepath} ({len(results)}ãƒ¬ãƒ¼ã‚¹)")


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---

def main():
    print("=" * 60)
    print("ğŸ UMA-Logic Pro - çµæœå–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    # å¯¾è±¡æ—¥ã‚’æ±ºå®šï¼ˆå¼•æ•°ã¾ãŸã¯è‡ªå‹•åˆ¤å®šï¼‰
    import sys
    if len(sys.argv) > 1:
        try:
            target_date = datetime.strptime(sys.argv[1], "%Y%m%d")
        except ValueError:
            print(f"[ERROR] æ—¥ä»˜å½¢å¼ãŒä¸æ­£ã§ã™: {sys.argv[1]} (YYYYMMDDå½¢å¼ã§æŒ‡å®š)")
            sys.exit(1)
    else:
        # è‡ªå‹•åˆ¤å®šï¼šGitHub Actions å®Ÿè¡Œæ™‚ã¯ç’°å¢ƒå¤‰æ•°ã§åˆ¤åˆ¥
        now = get_jst_now()
        print(f"[DEBUG] ç¾åœ¨æ™‚åˆ»(JST): {now.strftime('%Y-%m-%d %H:%M:%S %Z')}")
        
        if os.getenv('GITHUB_ACTIONS'):
            # GitHub Actions å®Ÿè¡Œæ™‚ã¯æ˜ç¤ºçš„ã«å‰æ—¥ã‚’æŒ‡å®š
            target_date = now - timedelta(days=1)
            print(f"[INFO] GitHub Actions æ¤œå‡º: å‰æ—¥({target_date.strftime('%Y-%m-%d')})ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™")
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ã¯æ™‚åˆ»ã§åˆ¤å®š
            target_date = now
            
            # 18æ™‚ä»¥é™ãªã‚‰å½“æ—¥ã®çµæœã‚’å–å¾—
            # 18æ™‚ä»¥å‰ãªã‚‰å‰æ—¥ã®çµæœã‚’å–å¾—
            if now.hour < 18:
                target_date = now - timedelta(days=1)
                print(f"[INFO] 18æ™‚å‰ã®ãŸã‚å‰æ—¥({target_date.strftime('%Y-%m-%d')})ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™")
            else:
                print(f"[INFO] 18æ™‚ä»¥é™ã®ãŸã‚å½“æ—¥({target_date.strftime('%Y-%m-%d')})ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™")
        
        print(f"[DEBUG] å¯¾è±¡æ—¥: {target_date.strftime('%Y-%m-%d (%A)')}")
    
    print(f"[INFO] å¯¾è±¡æ—¥: {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
    
    # ãƒ¬ãƒ¼ã‚¹IDå–å¾—ï¼ˆæ–°æ–¹å¼ã‚’å„ªå…ˆã€å¤±æ•—ã—ãŸã‚‰æ—§æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    race_ids = get_race_ids_for_date_v2(target_date)
    
    if not race_ids:
        print("[INFO] æ–°æ–¹å¼ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€æ—§æ–¹å¼ã‚’è©¦è¡Œ...")
        race_ids = get_race_ids_for_date(target_date)
    
    if not race_ids:
        print("[INFO] æœ¬æ—¥ã¯é–‹å‚¬ãŒãªã„ã‹ã€ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(0)
    
    # äºˆæƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆçš„ä¸­åˆ¤å®šç”¨ï¼‰
    pred_path = DATA_DIR / f"{PREDICTIONS_PREFIX}{target_date.strftime('%Y%m%d')}.json"
    predictions = {}
    if pred_path.exists():
        try:
            with open(pred_path, 'r', encoding='utf-8') as f:
                predictions = json.load(f)
            print(f"[INFO] äºˆæƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {pred_path}")
        except Exception as e:
            print(f"[WARN] äºˆæƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    
    # å±¥æ­´èª­ã¿è¾¼ã¿
    history = load_history()
    
    # çµæœå–å¾—
    all_results = []
    
    for i, race_id in enumerate(race_ids):
        print(f"\n[{i+1}/{len(race_ids)}] ãƒ¬ãƒ¼ã‚¹ID: {race_id}")
        
        result = fetch_race_result(race_id)
        
        if result:
            all_results.append(result)
            
            # äºˆæƒ³ã¨ã®ç…§åˆ
            if predictions:
                pred_race = next(
                    (r for r in predictions.get("races", [])
                     if r.get("venue") == result.get("venue") and
                        r.get("race_num") == result.get("race_num")),
                    None
                )
                if pred_race:
                    pred_race["date"] = predictions.get("date", target_date.strftime("%Y-%m-%d"))
                    update_history(pred_race, result, history)
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
        if i < len(race_ids) - 1:
            time.sleep(REQUEST_INTERVAL)
    
    # ä¿å­˜
    if all_results:
        save_results(all_results, target_date)
        save_history(history)
        print(f"\n[SUCCESS] âœ… å…¨{len(all_results)}ãƒ¬ãƒ¼ã‚¹ã®çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")
    else:
        print("\n[WARN] çµæœã‚’å–å¾—ã§ããŸãƒ¬ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # === è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– ===
    try:
        from archive_manager import AutoArchiver
        archiver = AutoArchiver()
        archiver.archive_today_results()
        print("[INFO] æœ¬æ—¥ã®çµæœã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"[WARN] ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    print("=" * 60)
    print("å‡¦ç†å®Œäº†")
    print("=" * 60)


if __name__ == "__main__":
    main()
